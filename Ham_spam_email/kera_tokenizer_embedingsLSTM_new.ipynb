{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/titli/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing function from a different ipny \n",
    "import pandas as pd\n",
    "import collections\n",
    "from keras.initializers import Constant\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import operator\n",
    "from itertools import product\n",
    "import sys\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from functools import reduce\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "pd.options.display.max_columns = 1000\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Activation, Input, BatchNormalization, MaxPooling1D, Bidirectional,LSTM\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPool1D, Flatten , Embedding, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "#https://www.amazon.com/Neural-Networks-Deep-Learning-Textbook/dp/3319944622/ref=cm_cr_arp_d_product_top?ie=UTF8\n",
    "#https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_NUM_WORDS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>isspam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Confidential :Soma:, Ci@lis, :P:ntermin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>¢Â ÇªÁüÇÑ ¼³³¯ ÀÌº¥Æ® ÀÀ¸ðÇØ¼­ ºÎ¸ð´Ô²² ¼±¹°ÇÏ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_na_</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>StOck 0ppurtunities - their sh0Oting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>All your prescr[iption needs right here</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Subject  isspam\n",
       "0            Confidential :Soma:, Ci@lis, :P:ntermin       1\n",
       "1  ¢Â ÇªÁüÇÑ ¼³³¯ ÀÌº¥Æ® ÀÀ¸ðÇØ¼­ ºÎ¸ð´Ô²² ¼±¹°ÇÏ...       1\n",
       "2                                               _na_       1\n",
       "3               StOck 0ppurtunities - their sh0Oting       1\n",
       "6            All your prescr[iption needs right here       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_data = pd.read_csv('subject_spam.csv', index_col=0, encoding='utf8', engine='python')\n",
    "keras_data.fillna(\"_na_\", inplace = True)\n",
    "keras_data = keras_data.drop(keras_data[keras_data.isspam == \"_na_\"].index)\n",
    "spammer = {'spam ': 1,'ham ': 0} \n",
    "keras_data.isspam = [spammer[item] for item in keras_data.isspam] \n",
    "spam_index = keras_data[keras_data.isspam ==1].index\n",
    "ham_index = keras_data[keras_data.isspam == 0].index\n",
    "new_index = np.concatenate((spam_index[:10000], ham_index[:9997]), axis=0)\n",
    "keras_data_new = keras_data.iloc[new_index]\n",
    "labels = keras_data_new.isspam\n",
    "keras_data_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(os.path.join('glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15425 unique tokens.\n",
      "Shape of data tensor: (19997, 1000)\n",
      "Shape of label tensor: (19997, 2)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(keras_data_new.Subject)\n",
    "sequences = tokenizer.texts_to_sequences(keras_data_new.Subject)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "print('Training model.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_edit(out):\n",
    "    loss_list = [s for s in out.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in out.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in out.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in out.history.keys() if 'acc' in s and 'val' in s]\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    for l in loss_list:\n",
    "        train_loss .append(out.history[l])\n",
    "    for l in val_loss_list:\n",
    "        val_loss .append(out.history[l])\n",
    "    for l in acc_list:\n",
    "        train_acc.append(out.history[l])\n",
    "    for l in val_acc_list:\n",
    "        val_acc .append(out.history[l])\n",
    "    line  = { 'train_loss': reduce(operator.concat, train_loss)[-1],'val_loss': reduce(operator.concat, val_loss)[-1], \\\n",
    "             'train_acc': reduce(operator.concat, train_acc)[-1], 'val_acc': reduce(operator.concat, val_acc)[-1] }\n",
    "    return (line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_LSTM(fl1=16, fl2=16, fl3=16, dl=16, optimizer= 'RMSprop', kl = 5, layer =1): \n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Bidirectional(LSTM(units = fl1, return_sequences=True))(embedded_sequences)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dense(units=dl, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    preds = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss= 'categorical_crossentropy',\n",
    "              optimizer= optimizer,\n",
    "              metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1361s 85ms/step - loss: 0.3575 - acc: 0.8491 - val_loss: 0.2686 - val_acc: 0.8935\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1282s 80ms/step - loss: 0.2497 - acc: 0.8979 - val_loss: 0.2354 - val_acc: 0.9035\n",
      "{'train_loss': 0.24968781153044933, 'val_loss': 0.2354417315633722, 'train_acc': 0.8978622327790974, 'val_acc': 0.9034758689821467}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1254s 78ms/step - loss: 0.3523 - acc: 0.8503 - val_loss: 0.2630 - val_acc: 0.8970\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1266s 79ms/step - loss: 0.2355 - acc: 0.9021 - val_loss: 0.2264 - val_acc: 0.9127\n",
      "{'train_loss': 0.2354699908226546, 'val_loss': 0.22638070953729958, 'train_acc': 0.9021127641029635, 'val_acc': 0.9127281820455114}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1270s 79ms/step - loss: 0.6184 - acc: 0.6933 - val_loss: 0.5080 - val_acc: 0.8050\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1255s 78ms/step - loss: 0.4488 - acc: 0.8065 - val_loss: 0.3855 - val_acc: 0.8425\n",
      "{'train_loss': 0.4488087955795686, 'val_loss': 0.3854703269926838, 'train_acc': 0.8065383172822097, 'val_acc': 0.8424606151537885}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1221s 76ms/step - loss: 0.3186 - acc: 0.8664 - val_loss: 0.2374 - val_acc: 0.9092\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1200s 75ms/step - loss: 0.2087 - acc: 0.9147 - val_loss: 0.2080 - val_acc: 0.9137\n",
      "{'train_loss': 0.20869916702996405, 'val_loss': 0.20801371655320489, 'train_acc': 0.9147393423954477, 'val_acc': 0.9137284321229319}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1277s 80ms/step - loss: 0.3514 - acc: 0.8493 - val_loss: 0.2835 - val_acc: 0.8857\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1285s 80ms/step - loss: 0.2377 - acc: 0.9026 - val_loss: 0.2229 - val_acc: 0.9122\n",
      "{'train_loss': 0.23772201966342962, 'val_loss': 0.22292958445163272, 'train_acc': 0.9026128265809709, 'val_acc': 0.9122280570142536}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1276s 80ms/step - loss: 0.3407 - acc: 0.8562 - val_loss: 0.2567 - val_acc: 0.8970\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1280s 80ms/step - loss: 0.2236 - acc: 0.9078 - val_loss: 0.2111 - val_acc: 0.9172\n",
      "{'train_loss': 0.22362651783148188, 'val_loss': 0.2111334524868011, 'train_acc': 0.9078009750995357, 'val_acc': 0.9172293073268317}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1255s 78ms/step - loss: 0.5949 - acc: 0.7112 - val_loss: 0.4741 - val_acc: 0.8235\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1240s 78ms/step - loss: 0.4242 - acc: 0.8185 - val_loss: 0.3774 - val_acc: 0.8380\n",
      "{'train_loss': 0.4241618703776471, 'val_loss': 0.3773562935970222, 'train_acc': 0.8184773096860625, 'val_acc': 0.8379594898724682}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1248s 78ms/step - loss: 0.3043 - acc: 0.8701 - val_loss: 0.2335 - val_acc: 0.9080\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1241s 78ms/step - loss: 0.1934 - acc: 0.9202 - val_loss: 0.1981 - val_acc: 0.9205\n",
      "{'train_loss': 0.1934047328478911, 'val_loss': 0.1980822958940862, 'train_acc': 0.9202400300037504, 'val_acc': 0.9204801200449124}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1290s 81ms/step - loss: 0.3472 - acc: 0.8547 - val_loss: 0.2610 - val_acc: 0.8967\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1286s 80ms/step - loss: 0.2306 - acc: 0.9047 - val_loss: 0.2154 - val_acc: 0.9130\n",
      "{'train_loss': 0.23063948596622186, 'val_loss': 0.21536232464371338, 'train_acc': 0.9046755844257015, 'val_acc': 0.9129782445760452}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1267s 79ms/step - loss: 0.3368 - acc: 0.8569 - val_loss: 0.2497 - val_acc: 0.9022\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1284s 80ms/step - loss: 0.2207 - acc: 0.9096 - val_loss: 0.2197 - val_acc: 0.9167\n",
      "{'train_loss': 0.2206577612081309, 'val_loss': 0.21974856928680264, 'train_acc': 0.909613701712714, 'val_acc': 0.9167291823104787}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1270s 79ms/step - loss: 0.6421 - acc: 0.6674 - val_loss: 0.5591 - val_acc: 0.7837\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1215s 76ms/step - loss: 0.4756 - acc: 0.8026 - val_loss: 0.4066 - val_acc: 0.8377\n",
      "{'train_loss': 0.47558493767846954, 'val_loss': 0.4065854065818887, 'train_acc': 0.8026003250331786, 'val_acc': 0.8377094273568392}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1209s 76ms/step - loss: 0.2988 - acc: 0.8755 - val_loss: 0.2433 - val_acc: 0.8972\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1182s 74ms/step - loss: 0.1919 - acc: 0.9229 - val_loss: 0.1989 - val_acc: 0.9240\n",
      "{'train_loss': 0.19192576374574993, 'val_loss': 0.19890789941389223, 'train_acc': 0.9228653581772227, 'val_acc': 0.9239809952488122}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1211s 76ms/step - loss: 0.3315 - acc: 0.8590 - val_loss: 0.2735 - val_acc: 0.8937\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1204s 75ms/step - loss: 0.2306 - acc: 0.9058 - val_loss: 0.2203 - val_acc: 0.9145\n",
      "{'train_loss': 0.23059995652944063, 'val_loss': 0.22025659847860188, 'train_acc': 0.9058007250906364, 'val_acc': 0.9144786196698186}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1206s 75ms/step - loss: 0.3217 - acc: 0.8635 - val_loss: 0.2541 - val_acc: 0.8972\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1192s 74ms/step - loss: 0.2097 - acc: 0.9126 - val_loss: 0.2156 - val_acc: 0.9125\n",
      "{'train_loss': 0.2096943042865916, 'val_loss': 0.2156487055724637, 'train_acc': 0.9125515689237638, 'val_acc': 0.9124781195447873}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1201s 75ms/step - loss: 0.6254 - acc: 0.7126 - val_loss: 0.5327 - val_acc: 0.7949\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1182s 74ms/step - loss: 0.4546 - acc: 0.8143 - val_loss: 0.3968 - val_acc: 0.8372\n",
      "{'train_loss': 0.4545838972943651, 'val_loss': 0.396841415176394, 'train_acc': 0.814289286145867, 'val_acc': 0.8372093023255814}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1198s 75ms/step - loss: 0.2934 - acc: 0.8754 - val_loss: 0.2648 - val_acc: 0.8845\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1182s 74ms/step - loss: 0.1906 - acc: 0.9197 - val_loss: 0.1993 - val_acc: 0.9237\n",
      "{'train_loss': 0.1905837935449213, 'val_loss': 0.19932039650112846, 'train_acc': 0.9197399674735824, 'val_acc': 0.9237309327480882}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1336s 83ms/step - loss: 0.3418 - acc: 0.8567 - val_loss: 0.2483 - val_acc: 0.8992\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - ETA: 1s - loss: 0.2341 - acc: 0.903 - 1291s 81ms/step - loss: 0.2341 - acc: 0.9034 - val_loss: 0.2134 - val_acc: 0.9162\n",
      "{'train_loss': 0.23411664442354954, 'val_loss': 0.2133988193882558, 'train_acc': 0.9034254281859738, 'val_acc': 0.916229057279221}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1462s 91ms/step - loss: 0.3314 - acc: 0.8600 - val_loss: 0.2426 - val_acc: 0.9077\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1318s 82ms/step - loss: 0.2169 - acc: 0.9127 - val_loss: 0.2006 - val_acc: 0.9237\n",
      "{'train_loss': 0.21694530191600644, 'val_loss': 0.20062518561928236, 'train_acc': 0.9126765845805231, 'val_acc': 0.9237309327480882}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1379s 86ms/step - loss: 0.6103 - acc: 0.7022 - val_loss: 0.4867 - val_acc: 0.8122\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1305s 82ms/step - loss: 0.4308 - acc: 0.8191 - val_loss: 0.3787 - val_acc: 0.8427\n",
      "{'train_loss': 0.43078653529474537, 'val_loss': 0.3786606387954618, 'train_acc': 0.8191023878059264, 'val_acc': 0.8427106776694173}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1340s 84ms/step - loss: 0.3047 - acc: 0.8702 - val_loss: 0.2365 - val_acc: 0.9017\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1318s 82ms/step - loss: 0.1908 - acc: 0.9209 - val_loss: 0.1944 - val_acc: 0.9260\n",
      "{'train_loss': 0.19078558231554324, 'val_loss': 0.19438855479690068, 'train_acc': 0.920927615951994, 'val_acc': 0.9259814953738434}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1351s 84ms/step - loss: 0.3204 - acc: 0.8634 - val_loss: 0.2423 - val_acc: 0.9027\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1455s 91ms/step - loss: 0.2141 - acc: 0.9138 - val_loss: 0.2017 - val_acc: 0.9257\n",
      "{'train_loss': 0.21405879004156012, 'val_loss': 0.20166568572475302, 'train_acc': 0.913801725200749, 'val_acc': 0.9257314328582146}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1317s 82ms/step - loss: 0.3306 - acc: 0.8590 - val_loss: 0.2416 - val_acc: 0.9085\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1283s 80ms/step - loss: 0.2061 - acc: 0.9161 - val_loss: 0.2016 - val_acc: 0.9207\n",
      "{'train_loss': 0.20606135111989074, 'val_loss': 0.20158017316619348, 'train_acc': 0.9160520064784581, 'val_acc': 0.9207301825605413}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1284s 80ms/step - loss: 0.6268 - acc: 0.6772 - val_loss: 0.5206 - val_acc: 0.7937\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1276s 80ms/step - loss: 0.4399 - acc: 0.8111 - val_loss: 0.3755 - val_acc: 0.8467\n",
      "{'train_loss': 0.4399032969167939, 'val_loss': 0.375535265565157, 'train_acc': 0.8111013876809107, 'val_acc': 0.8467116779194799}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1299s 81ms/step - loss: 0.2893 - acc: 0.8753 - val_loss: 0.2188 - val_acc: 0.9070\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1303s 81ms/step - loss: 0.1843 - acc: 0.9239 - val_loss: 0.2030 - val_acc: 0.9220\n",
      "{'train_loss': 0.1843036523447184, 'val_loss': 0.2029619497503451, 'train_acc': 0.9239279910063264, 'val_acc': 0.921980495123781}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1640s 103ms/step - loss: 0.3168 - acc: 0.8652 - val_loss: 0.2468 - val_acc: 0.8962\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1075s 67ms/step - loss: 0.2112 - acc: 0.9111 - val_loss: 0.1976 - val_acc: 0.9197\n",
      "{'train_loss': 0.2111505883170569, 'val_loss': 0.19762370926562056, 'train_acc': 0.911113889243606, 'val_acc': 0.9197299324980257}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1599s 100ms/step - loss: 0.3127 - acc: 0.8644 - val_loss: 0.2262 - val_acc: 0.9087\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1270s 79ms/step - loss: 0.1993 - acc: 0.9179 - val_loss: 0.2057 - val_acc: 0.9162\n",
      "{'train_loss': 0.19926822822434348, 'val_loss': 0.20567840559090875, 'train_acc': 0.917864733099088, 'val_acc': 0.916229057264316}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1312s 82ms/step - loss: 0.6227 - acc: 0.7160 - val_loss: 0.5227 - val_acc: 0.8075\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1294s 81ms/step - loss: 0.4483 - acc: 0.8159 - val_loss: 0.3833 - val_acc: 0.8442\n",
      "{'train_loss': 0.44834957993586433, 'val_loss': 0.38330110018850716, 'train_acc': 0.8159144892962609, 'val_acc': 0.8442110527631908}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1311s 82ms/step - loss: 0.2928 - acc: 0.8767 - val_loss: 0.2190 - val_acc: 0.9150\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1299s 81ms/step - loss: 0.1800 - acc: 0.9282 - val_loss: 0.1827 - val_acc: 0.9242\n",
      "{'train_loss': 0.18004241463712997, 'val_loss': 0.18269696043420952, 'train_acc': 0.9281785223227409, 'val_acc': 0.9242310577644411}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1302s 81ms/step - loss: 0.3140 - acc: 0.8674 - val_loss: 0.2385 - val_acc: 0.9072\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1301s 81ms/step - loss: 0.2116 - acc: 0.9114 - val_loss: 0.2234 - val_acc: 0.9132\n",
      "{'train_loss': 0.21163660805830287, 'val_loss': 0.2233685363614133, 'train_acc': 0.9114264283035379, 'val_acc': 0.9132283070767692}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1309s 82ms/step - loss: 0.3082 - acc: 0.8686 - val_loss: 0.2365 - val_acc: 0.9062\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1298s 81ms/step - loss: 0.1944 - acc: 0.9204 - val_loss: 0.2006 - val_acc: 0.9167\n",
      "{'train_loss': 0.19436868691775244, 'val_loss': 0.2006005202197647, 'train_acc': 0.9204275534441805, 'val_acc': 0.9167291823104787}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1300s 81ms/step - loss: 0.6227 - acc: 0.7172 - val_loss: 0.5169 - val_acc: 0.8015\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1267s 79ms/step - loss: 0.4478 - acc: 0.8092 - val_loss: 0.4012 - val_acc: 0.8312\n",
      "{'train_loss': 0.44784273356776755, 'val_loss': 0.40118077136510966, 'train_acc': 0.8092261532766102, 'val_acc': 0.8312078019504876}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1278s 80ms/step - loss: 0.2838 - acc: 0.8797 - val_loss: 0.2135 - val_acc: 0.9147\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1267s 79ms/step - loss: 0.1780 - acc: 0.9259 - val_loss: 0.1889 - val_acc: 0.9267\n",
      "{'train_loss': 0.17801708718993497, 'val_loss': 0.18886576020201912, 'train_acc': 0.9258657332166521, 'val_acc': 0.926731682935635}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1926s 120ms/step - loss: 0.3195 - acc: 0.8639 - val_loss: 0.2475 - val_acc: 0.9022\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1601s 100ms/step - loss: 0.2535 - acc: 0.8940 - val_loss: 0.2097 - val_acc: 0.9195\n",
      "{'train_loss': 0.25352338640104755, 'val_loss': 0.20966636908184025, 'train_acc': 0.8939867483211884, 'val_acc': 0.9194798699674919}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1594s 100ms/step - loss: 0.3219 - acc: 0.8639 - val_loss: 0.2517 - val_acc: 0.8955\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1652s 103ms/step - loss: 0.2346 - acc: 0.9024 - val_loss: 0.2019 - val_acc: 0.9215\n",
      "{'train_loss': 0.23457390479980997, 'val_loss': 0.2019009729528254, 'train_acc': 0.9024253031405408, 'val_acc': 0.9214803700925231}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1660s 104ms/step - loss: 0.6287 - acc: 0.6978 - val_loss: 0.5168 - val_acc: 0.8132\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1644s 103ms/step - loss: 0.4425 - acc: 0.8184 - val_loss: 0.3801 - val_acc: 0.8457\n",
      "{'train_loss': 0.4425306573430066, 'val_loss': 0.38012620038347084, 'train_acc': 0.8184148018576828, 'val_acc': 0.8457114278718691}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1692s 106ms/step - loss: 0.2897 - acc: 0.8782 - val_loss: 0.2073 - val_acc: 0.9130\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - ETA: 1s - loss: 0.2057 - acc: 0.914 - 1675s 105ms/step - loss: 0.2056 - acc: 0.9147 - val_loss: 0.1962 - val_acc: 0.9232\n",
      "{'train_loss': 0.20562949019855284, 'val_loss': 0.19622023879483091, 'train_acc': 0.9146768346043256, 'val_acc': 0.9232308077019254}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - ETA: 2s - loss: 0.3236 - acc: 0.865 - 2890s 181ms/step - loss: 0.3237 - acc: 0.8650 - val_loss: 0.2960 - val_acc: 0.8782\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1257s 79ms/step - loss: 0.2258 - acc: 0.9061 - val_loss: 0.2183 - val_acc: 0.9100\n",
      "{'train_loss': 0.2257649445002087, 'val_loss': 0.21833431556966193, 'train_acc': 0.9061132641580197, 'val_acc': 0.9099774943884983}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1203s 75ms/step - loss: 0.3070 - acc: 0.8707 - val_loss: 0.2218 - val_acc: 0.9155\n",
      "Epoch 2/2\n",
      "10432/15998 [==================>...........] - ETA: 7:32 - loss: 0.1955 - acc: 0.9198"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layers = [1]\n",
    "fl1s = [16,32,64, 128]\n",
    "fl2s = [0]\n",
    "fl3s = [0]\n",
    "kls = [0]\n",
    "dls = [16,32,64,128]\n",
    "#sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "optimizers = ['RMSprop', 'Adam', 'SGD', 'Nadam'] #, 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "loss_data = []\n",
    "params = []\n",
    "for fl1, fl2, fl3, kl, dl, optimizer,layer in product(fl1s,fl2s,fl3s,kls,dls,optimizers,layers):\n",
    "    kwargs = dict(fl1=fl1, fl2= fl2, fl3=fl3, kl=kl, dl=dl, optimizer= ''.join(optimizer), layer=layers)\n",
    "    params.append(kwargs)\n",
    "    model = embedding_LSTM(**kwargs)\n",
    "    #model.fit(x_train, y_train)\n",
    "    history = model.fit(x_train, y_train, batch_size= 16, epochs=2, validation_data=(x_val, y_val))\n",
    "    loss_data.append(loss_edit(history))\n",
    "    print(loss_edit(history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_pd = pd.DataFrame(params)\n",
    "loss_pd= pd.DataFrame(loss_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_LSTM = pd.concat([params_pd, loss_pd], axis=1, join='inner')\n",
    "embeddings_LSTM.to_csv('embeddings_LSTM_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dl</th>\n",
       "      <th>fl1</th>\n",
       "      <th>fl2</th>\n",
       "      <th>fl3</th>\n",
       "      <th>kl</th>\n",
       "      <th>layer</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.928179</td>\n",
       "      <td>0.180042</td>\n",
       "      <td>0.924231</td>\n",
       "      <td>0.182697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.925866</td>\n",
       "      <td>0.178017</td>\n",
       "      <td>0.926732</td>\n",
       "      <td>0.188866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.920928</td>\n",
       "      <td>0.190786</td>\n",
       "      <td>0.925981</td>\n",
       "      <td>0.194389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.914677</td>\n",
       "      <td>0.205629</td>\n",
       "      <td>0.923231</td>\n",
       "      <td>0.196220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.911114</td>\n",
       "      <td>0.211151</td>\n",
       "      <td>0.919730</td>\n",
       "      <td>0.197624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.920240</td>\n",
       "      <td>0.193405</td>\n",
       "      <td>0.920480</td>\n",
       "      <td>0.198082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.922865</td>\n",
       "      <td>0.191926</td>\n",
       "      <td>0.923981</td>\n",
       "      <td>0.198908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.919740</td>\n",
       "      <td>0.190584</td>\n",
       "      <td>0.923731</td>\n",
       "      <td>0.199320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.920428</td>\n",
       "      <td>0.194369</td>\n",
       "      <td>0.916729</td>\n",
       "      <td>0.200601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.912677</td>\n",
       "      <td>0.216945</td>\n",
       "      <td>0.923731</td>\n",
       "      <td>0.200625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.916052</td>\n",
       "      <td>0.206061</td>\n",
       "      <td>0.920730</td>\n",
       "      <td>0.201580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.913802</td>\n",
       "      <td>0.214059</td>\n",
       "      <td>0.925731</td>\n",
       "      <td>0.201666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.902425</td>\n",
       "      <td>0.234574</td>\n",
       "      <td>0.921480</td>\n",
       "      <td>0.201901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.923928</td>\n",
       "      <td>0.184304</td>\n",
       "      <td>0.921980</td>\n",
       "      <td>0.202962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.917865</td>\n",
       "      <td>0.199268</td>\n",
       "      <td>0.916229</td>\n",
       "      <td>0.205678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.914739</td>\n",
       "      <td>0.208699</td>\n",
       "      <td>0.913728</td>\n",
       "      <td>0.208014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.893987</td>\n",
       "      <td>0.253523</td>\n",
       "      <td>0.919480</td>\n",
       "      <td>0.209666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.907801</td>\n",
       "      <td>0.223627</td>\n",
       "      <td>0.917229</td>\n",
       "      <td>0.211133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.903425</td>\n",
       "      <td>0.234117</td>\n",
       "      <td>0.916229</td>\n",
       "      <td>0.213399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.904676</td>\n",
       "      <td>0.230639</td>\n",
       "      <td>0.912978</td>\n",
       "      <td>0.215362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.912552</td>\n",
       "      <td>0.209694</td>\n",
       "      <td>0.912478</td>\n",
       "      <td>0.215649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.906113</td>\n",
       "      <td>0.225765</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>0.218334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.909614</td>\n",
       "      <td>0.220658</td>\n",
       "      <td>0.916729</td>\n",
       "      <td>0.219749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.905801</td>\n",
       "      <td>0.230600</td>\n",
       "      <td>0.914479</td>\n",
       "      <td>0.220257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.902613</td>\n",
       "      <td>0.237722</td>\n",
       "      <td>0.912228</td>\n",
       "      <td>0.222930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.911426</td>\n",
       "      <td>0.211637</td>\n",
       "      <td>0.913228</td>\n",
       "      <td>0.223369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.902113</td>\n",
       "      <td>0.235470</td>\n",
       "      <td>0.912728</td>\n",
       "      <td>0.226381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.897862</td>\n",
       "      <td>0.249688</td>\n",
       "      <td>0.903476</td>\n",
       "      <td>0.235442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.811101</td>\n",
       "      <td>0.439903</td>\n",
       "      <td>0.846712</td>\n",
       "      <td>0.375535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.818477</td>\n",
       "      <td>0.424162</td>\n",
       "      <td>0.837959</td>\n",
       "      <td>0.377356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.819102</td>\n",
       "      <td>0.430787</td>\n",
       "      <td>0.842711</td>\n",
       "      <td>0.378661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.818415</td>\n",
       "      <td>0.442531</td>\n",
       "      <td>0.845711</td>\n",
       "      <td>0.380126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.815914</td>\n",
       "      <td>0.448350</td>\n",
       "      <td>0.844211</td>\n",
       "      <td>0.383301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.806538</td>\n",
       "      <td>0.448809</td>\n",
       "      <td>0.842461</td>\n",
       "      <td>0.385470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.814289</td>\n",
       "      <td>0.454584</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.396841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.809226</td>\n",
       "      <td>0.447843</td>\n",
       "      <td>0.831208</td>\n",
       "      <td>0.401181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.802600</td>\n",
       "      <td>0.475585</td>\n",
       "      <td>0.837709</td>\n",
       "      <td>0.406585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dl  fl1  fl2  fl3  kl layer optimizer  train_acc  train_loss   val_acc  \\\n",
       "27   64   32    0    0   0   [1]     Nadam   0.928179    0.180042  0.924231   \n",
       "31  128   32    0    0   0   [1]     Nadam   0.925866    0.178017  0.926732   \n",
       "19   16   32    0    0   0   [1]     Nadam   0.920928    0.190786  0.925981   \n",
       "35   16   64    0    0   0   [1]     Nadam   0.914677    0.205629  0.923231   \n",
       "24   64   32    0    0   0   [1]   RMSprop   0.911114    0.211151  0.919730   \n",
       "7    32   16    0    0   0   [1]     Nadam   0.920240    0.193405  0.920480   \n",
       "11   64   16    0    0   0   [1]     Nadam   0.922865    0.191926  0.923981   \n",
       "15  128   16    0    0   0   [1]     Nadam   0.919740    0.190584  0.923731   \n",
       "29  128   32    0    0   0   [1]      Adam   0.920428    0.194369  0.916729   \n",
       "17   16   32    0    0   0   [1]      Adam   0.912677    0.216945  0.923731   \n",
       "21   32   32    0    0   0   [1]      Adam   0.916052    0.206061  0.920730   \n",
       "20   32   32    0    0   0   [1]   RMSprop   0.913802    0.214059  0.925731   \n",
       "33   16   64    0    0   0   [1]      Adam   0.902425    0.234574  0.921480   \n",
       "23   32   32    0    0   0   [1]     Nadam   0.923928    0.184304  0.921980   \n",
       "25   64   32    0    0   0   [1]      Adam   0.917865    0.199268  0.916229   \n",
       "3    16   16    0    0   0   [1]     Nadam   0.914739    0.208699  0.913728   \n",
       "32   16   64    0    0   0   [1]   RMSprop   0.893987    0.253523  0.919480   \n",
       "5    32   16    0    0   0   [1]      Adam   0.907801    0.223627  0.917229   \n",
       "16   16   32    0    0   0   [1]   RMSprop   0.903425    0.234117  0.916229   \n",
       "8    64   16    0    0   0   [1]   RMSprop   0.904676    0.230639  0.912978   \n",
       "13  128   16    0    0   0   [1]      Adam   0.912552    0.209694  0.912478   \n",
       "36   32   64    0    0   0   [1]   RMSprop   0.906113    0.225765  0.909977   \n",
       "9    64   16    0    0   0   [1]      Adam   0.909614    0.220658  0.916729   \n",
       "12  128   16    0    0   0   [1]   RMSprop   0.905801    0.230600  0.914479   \n",
       "4    32   16    0    0   0   [1]   RMSprop   0.902613    0.237722  0.912228   \n",
       "28  128   32    0    0   0   [1]   RMSprop   0.911426    0.211637  0.913228   \n",
       "1    16   16    0    0   0   [1]      Adam   0.902113    0.235470  0.912728   \n",
       "0    16   16    0    0   0   [1]   RMSprop   0.897862    0.249688  0.903476   \n",
       "22   32   32    0    0   0   [1]       SGD   0.811101    0.439903  0.846712   \n",
       "6    32   16    0    0   0   [1]       SGD   0.818477    0.424162  0.837959   \n",
       "18   16   32    0    0   0   [1]       SGD   0.819102    0.430787  0.842711   \n",
       "34   16   64    0    0   0   [1]       SGD   0.818415    0.442531  0.845711   \n",
       "26   64   32    0    0   0   [1]       SGD   0.815914    0.448350  0.844211   \n",
       "2    16   16    0    0   0   [1]       SGD   0.806538    0.448809  0.842461   \n",
       "14  128   16    0    0   0   [1]       SGD   0.814289    0.454584  0.837209   \n",
       "30  128   32    0    0   0   [1]       SGD   0.809226    0.447843  0.831208   \n",
       "10   64   16    0    0   0   [1]       SGD   0.802600    0.475585  0.837709   \n",
       "\n",
       "    val_loss  \n",
       "27  0.182697  \n",
       "31  0.188866  \n",
       "19  0.194389  \n",
       "35  0.196220  \n",
       "24  0.197624  \n",
       "7   0.198082  \n",
       "11  0.198908  \n",
       "15  0.199320  \n",
       "29  0.200601  \n",
       "17  0.200625  \n",
       "21  0.201580  \n",
       "20  0.201666  \n",
       "33  0.201901  \n",
       "23  0.202962  \n",
       "25  0.205678  \n",
       "3   0.208014  \n",
       "32  0.209666  \n",
       "5   0.211133  \n",
       "16  0.213399  \n",
       "8   0.215362  \n",
       "13  0.215649  \n",
       "36  0.218334  \n",
       "9   0.219749  \n",
       "12  0.220257  \n",
       "4   0.222930  \n",
       "28  0.223369  \n",
       "1   0.226381  \n",
       "0   0.235442  \n",
       "22  0.375535  \n",
       "6   0.377356  \n",
       "18  0.378661  \n",
       "34  0.380126  \n",
       "26  0.383301  \n",
       "2   0.385470  \n",
       "14  0.396841  \n",
       "30  0.401181  \n",
       "10  0.406585  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_LSTM_sort = embeddings_LSTM.sort_values(['val_loss'])\n",
    "embeddings_LSTM_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
