{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Fake Reviewers\n",
    "###  The problem statement\n",
    "Online reviews are generated in an effort to improve and enhance their busines-\n",
    "ses, online retailers and service. Online reviews are helpful but blindly trust of\n",
    "these reviews is dangerous for both the seller and buyer. Many people make\n",
    "read online reviews before placing any order; however, the reviews may be faked\n",
    "for profit or gain. So to its necessary to decide whether the review is fake or\n",
    "truth more cautiously. This chapter deals with finding fake reviewers based on\n",
    "some characteristics features.<br>\n",
    "In the literature, fake reviews are categorized into three groups, proposed by\n",
    "Dixit et al. [2]: (1) Untruthful Reviews of the main concern of this paper, (2)\n",
    "Reviews on Brands of where the comments are only concerned with the brand\n",
    "or the seller of the product and fail to review the product, and (3) Non-Reviews\n",
    "are those reviews that contain either unrelated text or advertisements. The first\n",
    "category, untruthful reviews, is of most concern as they undermine the integrity\n",
    "of the online review system. Detection of this type 1 review is a challenging\n",
    "task. It is impossible, to distinguish between fake and real reviews by manually\n",
    "reading them. As a human judge it is difficult to confidently ascertain which\n",
    "review is fake and which is authentic.\n",
    "\n",
    "###  Data science to identify fake reviewers\n",
    "As of 2014 there were over 18 million reviews created on Yelp. Online reviews\n",
    "are constantly being generated on various web sites across the Internet. Hence,\n",
    "Big Data techniques are needed to address the problem of fake reviewers. Big\n",
    "Data, is often quantified with the Four V's (1) Volume of the sheer size and scale\n",
    "of the data, (2) Velocity of the rate at which new data is created and consu-\n",
    "med by processing engines, (3) Variety of the different formats that data may\n",
    "be stored in, and (4) Veracity of the quality level of the data. The Volume and\n",
    "Velocity of online reviews are noted by merely visiting e-commerce and custo-\n",
    "mer rating sites, such as Yelp and Amazon. There is great Variety across the\n",
    "possible industry sectors for reviews (such as hotels, restaurants, e-commerce,\n",
    "home services, etc.), along with the multiplicity of languages that reviews are\n",
    "written in. Veracity is a problem with online reviews, the vast majority of re-\n",
    "views are unlabeled, which means it is not easily known whether the review\n",
    "is fake or not. Thus, review spam detection is a Big Data problem, as there\n",
    "are numerous challenges when analyzing and classifying varying reviews from\n",
    "disconnected sources.\n",
    "### What kind of problem is it ?\n",
    "The problem is classification problem (1) Fake reviewer (category 1 in this study) (2) Real reviewer (category 0 in this study). The training dataset is hand labeled to fake or real review and is verified from Review Skeptic (http://reviewskeptic.com/) . Based on the training set the test set is classified to fake and real reviews.\n",
    "\n",
    "### Data Acquisition and Cleaning\n",
    "The dataset is acquire from Yelp (https://www.yelp.com/dataset). Yelp Dataset JSON contains files composed of a single object type, one JSON-object per-line.\n",
    "<br>\n",
    "business.json : Contains business data including location data, attributes, and categories.\n",
    "<br>\n",
    "review.json: Contains full review text data including the user_id that wrote the review and the business_id the review is written for.\n",
    "<br>\n",
    "user.json: User data including the user's friend mapping and all the metadata associated with the user.\n",
    "<br>\n",
    "checkin.json: Checkins on a business.\n",
    "<br>\n",
    "photo.json: Contains photo data including the caption and classification (one of \"food\", \"drink\", \"menu\", \"inside\" or \"outside\").\n",
    "<br> \n",
    "tip.json: Tips written by a user on a business. Tips are shorter than reviews and tend to convey quick suggestions.\n",
    "In this study checkin.json, photo.json and tip.json are not used. 200 user ids are extracted and review texts are hand labled as fake and real. Most of the user_ids have more than one review. However in this study on the first review of each users is labled. More details about acquiring and concatenating the data can be found in this IPython notebook. \n",
    "\n",
    "The categories of review for fake and real profiles are shown in Figure below. Most of the categories are related to food followed by hotel reviews, automobile reviews and so on. The categories for fake and real profiles have equal distribution.\n",
    "![alt text](1.png \"Categories of bussiness reviewed by fake reviewers\")\n",
    "![alt text](2.png \"Categories of bussiness reviewed by real reviewers\")\n",
    "\n",
    "### Feature Engineering\n",
    "Feature engineering is the helps in engineering pipelines and architectures designed to handle and transform raw data into something usable that can used for machine learning. This problem is a mix of categorical feature and cardinal (continuous) features along with text. The problem is tested with categorical feature and continuous features along with basic NLP analysis. \n",
    "#### Review length\n",
    "The average review length may be an important indication of reviewers with questionable intentions since about 80 % of spammers have no reviews longer than 135 words while more than 92% of reliable reviewers have an average review\n",
    "length of greater than 200 words. The distribution shows that fake reviewers write very short reviews as well as some long reviews too. Hence, Figure shows a bimodal distribution in comparison to real profiles.\n",
    "![alt text](3.png \"Categories of bussiness reviewed by fake reviewers\")\n",
    "#### Review length without stop word\n",
    "Real reviewers use more stop words to express emotions. Since fake reviewers use more direct approach so do not use much stop words. \n",
    "![alt text](4.png \"Categories of bussiness reviewed by real reviewers\")\n",
    "#### Number of nouns  and verbs in reviews\n",
    "When reviewers want to sound sincere but aren’t, they use more first-person pronouns like “I” and “me. Genuine reviews focus more heavily on describing situations with nouns, while fake reviews replace these with verbs. This “action” is supposed to make the reviews sound more convincing, but it ends up doing the opposite. In Figure there is bimodal distribution of verbs for fake reviewers.\n",
    "![alt text](5.png \"Categories of bussiness reviewed by fake reviewers\")\n",
    "![alt text](6.png \"Categories of bussiness reviewed by real reviewers\")\n",
    "#### Number of friends  and fans\n",
    "This dataset is not very complete and hence shows some fakew reviewers are having more fans than real reviewers. But in general fake reviewers have a low social profile.\n",
    "![alt text](7.png \"Categories of bussiness reviewed by fake reviewers\")\n",
    "#### Number of reviews and useful\n",
    "The number of times reviews have been marked useful is shown in the Figure. Fake reviewers write mostly one reviews compared to real reviewers who write more than one reviews. Also reviews from real reviewers are marked useful more often compared to fake. The labels used in the graph are described below. \n",
    "\n",
    "label=0 : Only one review.\n",
    "<br>\n",
    "label=1 : More than 1 and upto 20 reviews.\n",
    "<br>\n",
    "label=2 : More than 20 and upto 50 reviews.\n",
    "<br>\n",
    "label=3 : More than 50 and upto 100 reviews.\n",
    "<br>\n",
    "label=4 : More than 100 reviews.\n",
    "![alt text](8.png \"Categories of bussiness reviewed by real reviewers\")\n",
    "#### Distribution of actual bussiness stars and stas rating given by reviewer\n",
    "In case of a real profile 3.5, 4.0, 4.5 and 5 star bussiness received ratings more postive ratings than 1 and 2 stars. However in fake profile 3.5, 4.0 and 4.5 businesses are rated 1 stars by more than 7 fake reviewers. Also fake reviews tend to give more 1 star ratings than real reviewers.\n",
    "![alt text](9.png \"Categories of bussiness reviewed by real reviewers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and evaluation metric\n",
    "Once the data is pre-processed, they are fed to classification algorithm to build the model. In order to evaluate the performance of the model, the model is tested on the test dataset. Before making predictions on test dataset, we use the exact same pre-processing steps that we used for training dataset and apply them on the test dataset. Python’s scikit learn library are used for the machine learning approaches. The “pipeline\" functions is also used to combine all the steps, i.e. pre-processing and classifier learning steps into one. The 200 reviews are divided into 50% of fake and real reviews. The test-training split is made 20%-80%. So a total of 160 rows of data are used for training set and 40 rows of data used for test set. The fit of the model is tested based on confusion matrix.\n",
    "#### Confusion matrix\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. There are two possible predicted classes: \"yes\" and \"no\".The most basic terms are whole numbers (not rates):\n",
    "- true positives (TP): These are cases in which are predicted yes and are yes. \n",
    "- true negatives (TN): These are cases in which are predicted no and are no. \n",
    "- false positives (FP): These are cases in which are predicted yes but are no.(Also known as a \"Type I error.\")\n",
    "- false negatives (FN): These are cases in which are predicted no but are yes. (Also known as a \"Type II error.\")\n",
    "This is a list of rates that are often computed from a confusion matrix for a binary classifier:\n",
    "- Accuracy: Overall, how often is the classifier correct? $\\frac{(TP+TN)}{total}$. \n",
    "- Precision: When it predicts yes, how often is it correct? $\\frac{TP}{TP+FP}$\n",
    "- Recall: The ability of a model to find all the relevant cases within a dataset. $\\frac{TP}{TP+FN}$\n",
    "- F-Score: This is a weighted average of the true positive rate (recall) and precision. $2* \\frac{Precision*Recall}{Precision+Recall}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning methods:\n",
    "\n",
    "\n",
    "### Logistic regression:\n",
    "The logistic regression model arises from the desire to model the posterior probabilities of the $K$ classes via linear functions in $x$, while at the same time ensuring that they sum to one and remain in $[0,1]$. The model has the form\n",
    "\\begin{equation}\n",
    "log \\frac{Pr(G= K-1|X=x)}{Pr(G= K|X=x)} = \\beta_{(K-1)0} + \\beta_{(K-1)}^T x.\n",
    "\\end{equation}\n",
    "The model is specified in terms of $K-1$ logit transformations (reflecting the constraint that the probabilities sum to one). Although the model uses the last class as the denominator in the odds-ratios, the choice of denominator is arbitrary in that the estimates are equivariant under this choice. Since $Pr(G|X)$ completely specifies the conditional distribution, the multinomial distribution is appropriate.\n",
    "\n",
    "### Naive Bayes:\n",
    "Naive Bayes is a very simple classification algorithm that makes some strong assumptions about the independence of each input variable.There are two types of quantities that need to be calculated from the dataset for the naive Bayes model:\n",
    "Class Probabilities and Conditional Probabilities. The class probabilities for classes 0 and 1 are \n",
    "\\begin{equation}\n",
    "P(class=1) = count(class=1) / (count(class=0) + count(class=1))\n",
    "P(class=0) = count(class=0) / (count(class=0) + count(class=1))\n",
    "\\end{equation}    \n",
    "The conditional probabilities are the probability of each input value given each class value. To make predictions using Bayes Theorem.\n",
    "\\begin{equation}\n",
    "P(h|d) = (P(d|h) * P(h)) / P(d),\n",
    "\\end{equation}\n",
    "where:\n",
    "\n",
    "P(h|d) is the probability of hypothesis h given the data d. This is called the posterior probability.<br>\n",
    "P(d|h) is the probability of data d given that the hypothesis h was true.<br>\n",
    "P(h) is the probability of hypothesis h being true (regardless of the data). This is called the prior probability of h.<br>\n",
    "P(d) is the probability of the data (regardless of the hypothesis).<br>\n",
    "\n",
    "\n",
    "### Ensemble method\n",
    "The idea of ensemble learning is to build a prediction model by combining the strengths of a collection of simpler base models. Gradient Bagging and random forests are ensemble methods for classification, where a committee of trees each cast a vote for the predicted class. Stacking is a novel approach to combining the strengths of a number of fitted models. Ensemble learning can be broken down into two tasks: developing a population of base learners from the training data, and then combining them to form the composite predictor.\n",
    "#### Gradient Boosting\n",
    "\n",
    "#### Random Forest\n",
    "The essential idea in bagging is to average many noisy but approximately unbiased models, and hence reduce the variance. Trees are ideal candidates for bagging, since they can capture complex interaction structures in the data, and if grown sufficiently deep, have relatively low bias. Since trees are notoriously noisy, they benefit greatly from the averaging. Moreover, since each tree generated in bagging is identically distributed, the expectation of an average of B such trees is the same as the expectation of any one of them. The random forest algorithm is defined as an average of B identically distributed random variables, each with variance $\\sigma^2$, has variance $\\frac{1}{B} \\sigma^2$. If the variables are simply identically distributed, but not necessarily independent) with positive pairwise correlation $\\rho$, the variance of the average is $\\rho \\sigma + \\frac{1-\\rho}{B}\\sigma^2$. The idea in random forests is to improve the variance reduction of bagging by reducing the correlation between the trees, without increasing the variance too much. This is achieved in the tree-growing process through random selection of the input variables.\n",
    "\n",
    "##### Feature Selection\n",
    "The random forest model has 43 hyperparameters to be optimized. A feature selection method is used as shown in Figure to find the most important features. The top 20 features in terms of their feature importances are shown in the figure. Out of these top 20 features, 13 have information about the review text, 2 are information on bussiness and 5 have information on reviewer charecterterestics.\n",
    "![alt text](15.png \"Categories of bussiness reviewed by real reviewers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tunning in machine learning models\n",
    "This section disusses chossing of the best parameter for the model based on accuracy score.\n",
    "#### Inverse of regulization: logistic regression:\n",
    "An inverse of regulization is used to improve the generalization performance, i.e., the performance on new, unseen data. In more specific terms, regularization is increasing  bias if our model suffers from (high) variance (i.e., it overfits the training data). On the other hand, too much bias will result in underfitting. Figure shows the accuracy score for a set of inverse of regulization values. 1, 10 and 100 have almost same accuracy score so a value of 1 is chosen to avoid underfitting.\n",
    "![alt text](11.png \"Categories of bussiness reviewed by fake reviewers\")\n",
    "#### Number of steps: gradient boosting\n",
    "The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. Figure shows that 30 or 40 steps gives the maximum accuracy.\n",
    "![alt text](12.png \"Categories of bussiness reviewed by fake reviewers\")\n",
    "#### Number of trees: random forest\n",
    "The number of trees in the forest. Figure shows that 70 or 100 trees gives maximum accuracy.\n",
    "![alt text](14.png \"Categories of bussiness reviewed by fake reviewers\")\n",
    "#### Number of trees: extra tree\n",
    "The number of trees in the forest. Figure shows that 70 gives the best estimate of maximum accuracy.\n",
    "![alt text](13.png \"Categories of bussiness reviewed by fake reviewers\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparisons\n",
    "Logistic Regression, Gaussian Naive Bayes, Random Forest, Gradient Boosting and Extra Randomized Trees classifiers to build a model to predict Fake reviewers. Results are shown for both the training set and testing set. The results\n",
    "of various evaluation metrics scores are shown in Tabel1  and Table 2 for all models. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model | Accuracy| Recall|Precision| F-score|\n",
    "---|---|---|---|---|\n",
    "<span style=\"color:magenta\">**Logistic Regression**</span>|0.68|0.65|0.68|0.67\n",
    "<span style=\"color:red\">Naive Bayes</span>|0.56|0.38|0.59|0.46|\n",
    "Gradient Boosting|0.69|\t0.69|0.70|0.69|\n",
    "Random Forest|0.94|\t0.94|0.95|0.94|\n",
    "<span style=\"color:green\">Extra Tree|0.97|0.96|0.97|0.97|</span>\n",
    "Table 1: Comparing all models: Red for the worst and green for the best model on training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model | Accuracy| Recall|Precision| F-score|\n",
    "---|---|---|---|---|\n",
    "<span style=\"color:magenta\">**Logistic Regression**</span>|0.70|0.55|0.79|0.65\n",
    "<span style=\"color:red\">Naive Bayes</span>|0.50|0.30|0.50|0.38\n",
    "Gradient Boosting|0.63|0.50|0.67|0.57\n",
    "<span style=\"color:green\">Random Forest</span>|0.73|\t0.75|0.71|0.73\n",
    "Extra Tree|0.68|0.65|0.68|0.67\n",
    "\n",
    "Table 2: Comparing all models: Red for the worst and green for the best model on testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Recommendations\n",
    "This section discusses the best model that can be used to predict fake reviewers. In statistics, a fit refers to how well  a target function is approximated. In supervised machine learning the unknown underlying mapping function is approximated from the output variables of training set. The target function is not always known so calculating the residual errors is not always the case in machine learning. In supervised learning approximations are made from samples of noisy training data. <br>\n",
    "Two senerios apper (1)Overfitting and (2) Underfitting. Overfitting refers to a model that models the training data too well. Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. Decision trees are a nonparametric machine learning algorithm that is very flexible and is subject to overfitting training data. This problem can be addressed by pruning a tree after it has learned in order to remove some of the detail it has picked up.<br>\n",
    "Underfitting refers to a model that can neither model the training data nor generalize to new data.\n",
    "An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data. A good fit in Machine Learning happends when the model is at the sweet spot between underfitting and overfitting which is very difficult to do in practice.<br>\n",
    "Table 1 shows accuracy precision, recall and Fscore for the five different models. The extra tree and random forest performs best on the training set. Logistic regression has an accuracy of 68% where as gradient boosting and Naive Bayes don't perform well. However, when comparing with Table 2 the best accuracy is predicted by random forest which is 73% followed by logistic regression of 70%. However it is to be noted that random forest suffers from overfitting. The decision trees have trained on noise so the accuracy has decreased from 94% on training set to 73% on test set. Overfitting is also noted in extra tree where the accuracy has decreased from 97% on training set to 68% on test set. The logistic regression model on the other hand has almost similar accuracies of 68% and 70% on the train and test set respetively. The F-score are also similar 67% and 65% respectively on train and test set. In this particular problem logistic regression is a good fit model performing with an accuracy of 70% on test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this project, The review text of users were merged with bussiness reviewed by the user. 200 review samples were labelled as fake and real. A quick summary of the exploratory data analysis reveals that 1. Fake reviewers use more verbs and nouns than real user. 2. Fake reviewers use less stop words.i.e express less emotions. 3. Fake reviewers give more 1 star reviews than real users. 4. Reviews of real users are more useful. 5. There is a disparity in star rating on bussiness and star rating by fake users. <br>\n",
    "After exploring all datasets, we used five different supervised classification algorithms (Logistic Regression, Gaussian Naive Bayes, Random Forest, Gradient Boosting and Extremely Randomized Trees) are used to train the predictive model by using 80% of the whole data. The remaining 20% was used to evaluate the model ie test data. Overall logistic regression performed well on both test and train data set with 70% accuracy. Random forest and extra tree performed well on training dataset but suffered from overfitting on test data set.<br>\n",
    "However it is to be noted that there are some limitations in the current model such as lack of complete data because only 200 samples were labelled for fake and real. It is also difficult to identify fake and real charecteretstics.  The model can be furthur improved in future with more sampled dataset and some reviews that are already marked as fake and real.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
