{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model\n",
    "from datetime import datetime\n",
    "from scipy.stats import skew\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "userdatapath = '/home/titli/Documents/Springboard1/test.csv'\n",
    "selecteddatapath = '/home/titli/Documents/Springboard1/test1.csv'\n",
    "reviewfile= '/home/titli/Documents/Springboard1/dataset/review.json'\n",
    "businessfile = '/home/titli/Documents/Springboard1/dataset/business.json'\n",
    "numsamples=1000\n",
    "rowsamples=200\n",
    "pd.options.display.max_columns = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>compliment_cool</th>\n",
       "      <th>compliment_cute</th>\n",
       "      <th>compliment_funny</th>\n",
       "      <th>compliment_hot</th>\n",
       "      <th>compliment_list</th>\n",
       "      <th>compliment_more</th>\n",
       "      <th>compliment_note</th>\n",
       "      <th>compliment_photos</th>\n",
       "      <th>compliment_plain</th>\n",
       "      <th>cool</th>\n",
       "      <th>fans</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.67</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.67</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.73</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   average_stars  review_count  compliment_cool  compliment_cute  \\\n",
       "0           4.67             8                0                0   \n",
       "1           1.00             1                0                0   \n",
       "2           1.67             2                0                0   \n",
       "3           3.00             2                0                0   \n",
       "4           3.73            48                1                0   \n",
       "\n",
       "   compliment_funny  compliment_hot  compliment_list  compliment_more  \\\n",
       "0                 0               0                0                0   \n",
       "1                 0               0                0                0   \n",
       "2                 0               0                0                0   \n",
       "3                 0               0                0                0   \n",
       "4                 1               2                0                0   \n",
       "\n",
       "   compliment_note  compliment_photos  compliment_plain  cool  fans  funny  \n",
       "0                0                  0                 1     0     0      0  \n",
       "1                0                  0                 0     0     0      0  \n",
       "2                0                  0                 0     0     0      0  \n",
       "3                0                  0                 0     1     0      0  \n",
       "4                1                  0                 1     2     3      6  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old=pd.read_csv(userdatapath, header=0,  index_col=0, nrows=numsamples)\n",
    "\n",
    "#100 points\n",
    "\n",
    "selectedrows=[0,8,12,15,19,25,28,29,45,47,53,54,65,71,85,87,94,101,106,114,126,127,174,175, \\\n",
    "              176,177,178,193,197,206,209,210,222,224,227,244,248,250,255,258, \\\n",
    "              275,276,279,284,289,290,303,309,313,326,331,332,338,349,356,364,366,381,387,390,392,397,\\\n",
    "              404,408,418,444,445,455,470,474,480,482,489,495,497,500,507,517,520,529, \\\n",
    "              1,2,3,4,5,6,7,9,10,13,17,18,21,24,26,27,30,31,32,33,34,35, \\\n",
    "              183,184,185,186,187,190,194,196,199,201,205,208,232,235,240,249,266,269,\\\n",
    "              327,328,329,333,334,335, 336,337,339,340,341,342,343,345,346,347,350,351,352,353,359,360,362,\\\n",
    "              363,365,367,368,369,627,374,376,380,382,383,385,386,388, \\\n",
    "              128,136,142,149,153,155,167,268,274,555,569,582,583,591,597,601,611,614,622,631, \\\n",
    "              36,37,38,39,40,42,43,44,270,273,389,391,393,394,395,396,400,471,473,479,623,624,626]                         \n",
    "              \n",
    "              \n",
    "#fake=174,175,176,177,178,193,197,206,209,210,222,224,227,244,248,250,255,258,268,274,275,276,279,284,289,290,303, \\\n",
    "#309,313,326\n",
    "#183,184,185,186,187,190,194,196,199,201,205,208,232,235,240,249,266,269,270,273,278,281,282,283,285,286,287,291,292,293,\\\n",
    "#295,296,324,325\n",
    "inter=[]\n",
    "for row in selectedrows:\n",
    "    line= old.iloc[row]\n",
    "    inter.append(line)\n",
    "old_1=pd.DataFrame(inter)\n",
    "old_1.set_index('user_id')\n",
    "old_1.head(10)\n",
    "old_1.to_csv(selecteddatapath)\n",
    "small= pd.read_csv(selecteddatapath, header=0)\n",
    "cols = 'average_stars \treview_count compliment_cool \tcompliment_cute \tcompliment_funny \t \\\n",
    "compliment_hot \tcompliment_list \tcompliment_more \tcompliment_note \tcompliment_photos \t \\\n",
    "compliment_plain \tcool  \tfans funny '.split()\n",
    "old_2=small[cols]\n",
    "old_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "def getreviews(user_ids):\n",
    "    \"\"\" gets one text review for each user in user_ids list\"\"\"\n",
    "    result=[]\n",
    "    with open(reviewfile) as f:\n",
    "         for line in f:\n",
    "             record = json.loads(line)\n",
    "             if record['user_id'] in user_ids: \n",
    "                result.append(record)\n",
    "             if len(result) >= 1000:  #numsamples\n",
    "                break\n",
    "    reviewdata = pd.DataFrame(result)\n",
    "    return reviewdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids=[]\n",
    "for counter, value in enumerate(selectedrows):\n",
    "    user_ids.append(old['user_id'].iloc[value])\n",
    "review_texts_all = getreviews(user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K1tLIHP5DLZG5NYIt83iYA</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-02</td>\n",
       "      <td>0</td>\n",
       "      <td>7AIRjMM5y80d8WlcyY32uQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Ken and Debra are amazing real estate agents! ...</td>\n",
       "      <td>0</td>\n",
       "      <td>RJaOM110aQlyfab_mYBw4w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PmgbmK6wG0S-Nndo1becBA</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-04-23</td>\n",
       "      <td>0</td>\n",
       "      <td>z8OYnWR_s62vtHp5GCmEWQ</td>\n",
       "      <td>1</td>\n",
       "      <td>Absolute worst service. I have never been trea...</td>\n",
       "      <td>1</td>\n",
       "      <td>akFg3cvYqbihkdYUIjTauQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rL-hprXkcvfYSL9kWBFglQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-02-23</td>\n",
       "      <td>0</td>\n",
       "      <td>JCUtZ8QvWDdjSEm7Km9W-w</td>\n",
       "      <td>5</td>\n",
       "      <td>I will be back.  Unlike the old Native New Yor...</td>\n",
       "      <td>0</td>\n",
       "      <td>ddhNJ-nbjwjHoSth6qXJ8g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p_22wrx3GmAzlbrGA6LyYg</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-03-10</td>\n",
       "      <td>0</td>\n",
       "      <td>-nj0swfsRUk83zgpWibT0A</td>\n",
       "      <td>5</td>\n",
       "      <td>Proud to support family owned restaurant.  I l...</td>\n",
       "      <td>0</td>\n",
       "      <td>ddhNJ-nbjwjHoSth6qXJ8g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_ZfjpSEO5ntk-1hbnwCR4g</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-10-10</td>\n",
       "      <td>1</td>\n",
       "      <td>fVL9vfsWydvzxVfmEiERtg</td>\n",
       "      <td>2</td>\n",
       "      <td>Stayed here for a few nights on a random getaw...</td>\n",
       "      <td>0</td>\n",
       "      <td>gb8PYSCKlvPbMWnqIWXmlQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool        date  funny               review_id  \\\n",
       "0  K1tLIHP5DLZG5NYIt83iYA     0  2017-10-02      0  7AIRjMM5y80d8WlcyY32uQ   \n",
       "1  PmgbmK6wG0S-Nndo1becBA     0  2016-04-23      0  z8OYnWR_s62vtHp5GCmEWQ   \n",
       "2  rL-hprXkcvfYSL9kWBFglQ     0  2014-02-23      0  JCUtZ8QvWDdjSEm7Km9W-w   \n",
       "3  p_22wrx3GmAzlbrGA6LyYg     0  2014-03-10      0  -nj0swfsRUk83zgpWibT0A   \n",
       "4  _ZfjpSEO5ntk-1hbnwCR4g     0  2015-10-10      1  fVL9vfsWydvzxVfmEiERtg   \n",
       "\n",
       "   stars                                               text  useful  \\\n",
       "0      5  Ken and Debra are amazing real estate agents! ...       0   \n",
       "1      1  Absolute worst service. I have never been trea...       1   \n",
       "2      5  I will be back.  Unlike the old Native New Yor...       0   \n",
       "3      5  Proud to support family owned restaurant.  I l...       0   \n",
       "4      2  Stayed here for a few nights on a random getaw...       0   \n",
       "\n",
       "                  user_id  \n",
       "0  RJaOM110aQlyfab_mYBw4w  \n",
       "1  akFg3cvYqbihkdYUIjTauQ  \n",
       "2  ddhNJ-nbjwjHoSth6qXJ8g  \n",
       "3  ddhNJ-nbjwjHoSth6qXJ8g  \n",
       "4  gb8PYSCKlvPbMWnqIWXmlQ  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_texts_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=[]\n",
    "for i in range(rowsamples):\n",
    "    data = review_texts_all[review_texts_all.user_id == user_ids[i]]\n",
    "    n = review_texts_all[review_texts_all.user_id == user_ids[i]].count()\n",
    "    if len(n) > 1:\n",
    "       data1 = data.iloc[0]\n",
    "       df1.append(data1)\n",
    "\n",
    "review_texts = pd.DataFrame(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = []\n",
    "for k in range(rowsamples):\n",
    "    df1.append({'rev_length': len(review_texts['text'].iloc[k]), 'rev_use': review_texts['useful'].iloc[k],\\\n",
    "               'rev_stars': review_texts['stars'].iloc[k], 'rev_date': review_texts['date'].iloc[k], \\\n",
    "               'rev_buss': review_texts['business_id'].iloc[k]})\n",
    "revlen=pd.DataFrame(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = []\n",
    "for k in range(rowsamples):\n",
    "    \n",
    "    label=0\n",
    "    Xlen=old_1['review_count'].iloc[k]\n",
    "    \n",
    "    if Xlen<2:\n",
    "       label=0\n",
    "    elif np.logical_and((Xlen>=2), (Xlen<=20)):\n",
    "       label=1\n",
    "    elif np.logical_and((Xlen>20), (Xlen<=50)):\n",
    "       label=2\n",
    "    elif np.logical_and((Xlen>50), (Xlen<=100)):\n",
    "       label=3\n",
    "    else:\n",
    "      label=4\n",
    "    df1.append({'rev_count_label': label})    \n",
    "Xlen_label=pd.DataFrame(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = []\n",
    "for k in range(0,rowsamples):\n",
    "    count=0\n",
    "    label=0\n",
    "    Xfrnd=(old_1['friends'].iloc[[k]].values).tolist()\n",
    "    dfrnd = (\", \".join(Xfrnd)).split(',')[1:-1]\n",
    "    if dfrnd is None:\n",
    "       break\n",
    "    else:\n",
    "       count=len(dfrnd)\n",
    "       if count==0:\n",
    "          label=0\n",
    "       elif np.logical_and((count>0), (count<=50)):\n",
    "          label=1\n",
    "       elif np.logical_and((count>50), (count<=100)):\n",
    "          label=2\n",
    "          \n",
    "       else:\n",
    "          label=3\n",
    "       df2.append({'friend_count': count,'friend_label': label})    \n",
    "frnd=pd.DataFrame(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "def getbusiness(business_ids):\n",
    "    \"\"\" gets stars review for each business in business_ids list\"\"\"\n",
    "    result=[]\n",
    "    with open(businessfile) as f:\n",
    "         for line in f:\n",
    "             record = json.loads(line)\n",
    "             if record['business_id'] in business_ids: \n",
    "                result.append(record)\n",
    "             if len(result) >= numsamples:\n",
    "                break\n",
    "    businessdata = pd.DataFrame(result)\n",
    "    return businessdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_ids = revlen['rev_buss'].iloc[:numsamples].values.tolist()\n",
    "businessstars = pd.DataFrame({'buss_star': getbusiness(business_ids)['stars'], 'buss_review': getbusiness(business_ids)['review_count']  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, word_tokenize\n",
    "english_stops = set(stopwords.words('english'))\n",
    "def alphalength(text):\n",
    "# Import WordNetLemmatizer\n",
    "    tokens = word_tokenize(text)\n",
    "    lower_tokens = [t.lower() for t in tokens]\n",
    "# Retain alphabetic words: alpha_only\n",
    "    alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "    length_alpha = len(alpha_only)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "    return(length_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alpha_length\n",
       "0            98\n",
       "1           139\n",
       "2            46\n",
       "3            35\n",
       "4           279"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=[]\n",
    "for i in range(rowsamples):\n",
    "    a1=alphalength(review_texts['text'].iloc[i])\n",
    "    df1.append(a1)\n",
    "Alphalength=pd.DataFrame({'alpha_length': df1})\n",
    "Alphalength.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, word_tokenize\n",
    "english_stops = set(stopwords.words('english'))\n",
    "def wordstop(text):\n",
    "# Import WordNetLemmatizer\n",
    "    tokens = word_tokenize(text)\n",
    "    lower_tokens = [t.lower() for t in tokens]\n",
    "# Retain alphabetic words: alpha_only\n",
    "    alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "       \n",
    "    # Remove all stop words: no_stops\n",
    "    no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "    length_alpha = len(no_stops)\n",
    "    \n",
    "# Print the 10 most common tokens\n",
    "    return(length_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nostop_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nostop_length\n",
       "0             46\n",
       "1             72\n",
       "2             18\n",
       "3             18\n",
       "4            137"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=[]\n",
    "for i in range(rowsamples):\n",
    "    a1=wordstop(review_texts['text'].iloc[i])\n",
    "    df1.append(a1)\n",
    "stopwords=pd.DataFrame({'nostop_length': df1})\n",
    "stopwords.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, word_tokenize\n",
    "english_stops = set(stopwords.words('english'))\n",
    "def wordlemmatize(text):\n",
    "# Import WordNetLemmatizer\n",
    "    tokens = word_tokenize(text)\n",
    "    lower_tokens = [t.lower() for t in tokens]\n",
    "# Retain alphabetic words: alpha_only\n",
    "    alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "       \n",
    "    # Remove all stop words: no_stops\n",
    "    no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "    \n",
    "    #  Instantiate the WordNetLemmatizer\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # Lemmatize all tokens into a new list: lemmatized\n",
    "    lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "    \n",
    "    length_lemmatize = len(lemmatized)\n",
    "    \n",
    "# Print the 10 most common tokens\n",
    "    return(length_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_lemmatize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length_lemmatize\n",
       "0                46\n",
       "1                72\n",
       "2                18\n",
       "3                18\n",
       "4               137"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=[]\n",
    "for i in range(rowsamples):\n",
    "    a1=wordlemmatize(review_texts['text'].iloc[i])\n",
    "    df1.append(a1)\n",
    "lemmatizewords=pd.DataFrame({'length_lemmatize': df1})\n",
    "lemmatizewords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, word_tokenize\n",
    "english_stops = set(stopwords.words('english'))\n",
    "def wordpos(text):\n",
    "# Import WordNetLemmatizer\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    lower_tokens = [t.lower() for t in tokens]\n",
    "    \n",
    "    counts = Counter([j for i,j in pos_tag(lower_tokens)])\n",
    "    \n",
    "\n",
    "    return (counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/titli/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:3787: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NN</th>\n",
       "      <th>NNP</th>\n",
       "      <th>NNS</th>\n",
       "      <th>PDT</th>\n",
       "      <th>POS</th>\n",
       "      <th>PRP</th>\n",
       "      <th>PRP$</th>\n",
       "      <th>RB</th>\n",
       "      <th>RBR</th>\n",
       "      <th>RBS</th>\n",
       "      <th>RP</th>\n",
       "      <th>VB</th>\n",
       "      <th>VBD</th>\n",
       "      <th>VBG</th>\n",
       "      <th>VBN</th>\n",
       "      <th>VBP</th>\n",
       "      <th>VBZ</th>\n",
       "      <th>WDT</th>\n",
       "      <th>WP</th>\n",
       "      <th>WRB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NN  NNP  NNS  PDT  POS   PRP  PRP$    RB  RBR  RBS   RP    VB   VBD  VBG  \\\n",
       "0  22  0.0  3.0  0.0  1.0   8.0   3.0   8.0  0.0  0.0  0.0   5.0   5.0  0.0   \n",
       "1  30  0.0  8.0  0.0  0.0   5.0   9.0   9.0  0.0  0.0  0.0   7.0  11.0  1.0   \n",
       "2  10  0.0  3.0  0.0  0.0   4.0   1.0   0.0  0.0  0.0  0.0   2.0   6.0  0.0   \n",
       "3   6  0.0  3.0  0.0  0.0   4.0   1.0   4.0  0.0  0.0  0.0   2.0   2.0  0.0   \n",
       "4  55  0.0  9.0  0.0  0.0  13.0   4.0  17.0  1.0  0.0  0.0  14.0  15.0  3.0   \n",
       "\n",
       "    VBN  VBP   VBZ  WDT   WP  WRB  \n",
       "0   4.0  1.0   3.0  1.0  0.0  1.0  \n",
       "1   4.0  3.0   1.0  1.0  0.0  1.0  \n",
       "2   2.0  2.0   1.0  0.0  1.0  0.0  \n",
       "3   0.0  1.0   1.0  0.0  0.0  0.0  \n",
       "4  10.0  6.0  16.0  1.0  2.0  1.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=[]\n",
    "for i in range(rowsamples):\n",
    "    a1=wordpos(review_texts['text'].iloc[i])\n",
    "    df1.append(a1)\n",
    "\n",
    "Netpos=pd.DataFrame(df1)\n",
    "Netpos.head()\n",
    "POS_columns = 'NN \tNNP \tNNS \tPDT \tPOS \t\\\n",
    "PRP \tPRP$ \tRB \tRBR \tRBS \tRP \t\tVB \tVBD \tVBG \tVBN \t\\\n",
    "VBP \tVBZ \tWDT \tWP \tWRB'.split()\n",
    "postotal=Netpos[POS_columns]\n",
    "postotal.fillna(value= 0.0, inplace=True)\n",
    "postotal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>compliment_cool</th>\n",
       "      <th>compliment_cute</th>\n",
       "      <th>compliment_funny</th>\n",
       "      <th>compliment_hot</th>\n",
       "      <th>compliment_list</th>\n",
       "      <th>compliment_more</th>\n",
       "      <th>compliment_note</th>\n",
       "      <th>compliment_photos</th>\n",
       "      <th>compliment_plain</th>\n",
       "      <th>cool</th>\n",
       "      <th>fans</th>\n",
       "      <th>funny</th>\n",
       "      <th>rev_length</th>\n",
       "      <th>rev_stars</th>\n",
       "      <th>rev_use</th>\n",
       "      <th>friend_count</th>\n",
       "      <th>friend_label</th>\n",
       "      <th>rev_count_label</th>\n",
       "      <th>buss_star</th>\n",
       "      <th>buss_review</th>\n",
       "      <th>alpha_length</th>\n",
       "      <th>length_lemmatize</th>\n",
       "      <th>NN</th>\n",
       "      <th>NNP</th>\n",
       "      <th>NNS</th>\n",
       "      <th>PDT</th>\n",
       "      <th>POS</th>\n",
       "      <th>PRP</th>\n",
       "      <th>PRP$</th>\n",
       "      <th>RB</th>\n",
       "      <th>RBR</th>\n",
       "      <th>RBS</th>\n",
       "      <th>RP</th>\n",
       "      <th>VB</th>\n",
       "      <th>VBD</th>\n",
       "      <th>VBG</th>\n",
       "      <th>VBN</th>\n",
       "      <th>VBP</th>\n",
       "      <th>VBZ</th>\n",
       "      <th>WDT</th>\n",
       "      <th>WP</th>\n",
       "      <th>WRB</th>\n",
       "      <th>Faker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.00000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.688800</td>\n",
       "      <td>8.280000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>534.855000</td>\n",
       "      <td>3.645000</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>13.735000</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>3.762500</td>\n",
       "      <td>347.730000</td>\n",
       "      <td>98.620000</td>\n",
       "      <td>47.720000</td>\n",
       "      <td>19.630000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>3.935000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>5.520000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>8.150000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.53000</td>\n",
       "      <td>5.015000</td>\n",
       "      <td>7.220000</td>\n",
       "      <td>1.735000</td>\n",
       "      <td>1.945000</td>\n",
       "      <td>2.680000</td>\n",
       "      <td>2.035000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.253741</td>\n",
       "      <td>16.662225</td>\n",
       "      <td>0.913935</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.913935</td>\n",
       "      <td>0.452353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.306995</td>\n",
       "      <td>0.787784</td>\n",
       "      <td>0.752617</td>\n",
       "      <td>0.944436</td>\n",
       "      <td>1.558499</td>\n",
       "      <td>5.567593</td>\n",
       "      <td>4.385621</td>\n",
       "      <td>536.602446</td>\n",
       "      <td>1.662164</td>\n",
       "      <td>1.424119</td>\n",
       "      <td>58.262832</td>\n",
       "      <td>0.685071</td>\n",
       "      <td>0.660421</td>\n",
       "      <td>0.896584</td>\n",
       "      <td>769.966779</td>\n",
       "      <td>101.510975</td>\n",
       "      <td>46.028302</td>\n",
       "      <td>18.610612</td>\n",
       "      <td>0.172478</td>\n",
       "      <td>4.260462</td>\n",
       "      <td>0.303911</td>\n",
       "      <td>0.537629</td>\n",
       "      <td>8.171503</td>\n",
       "      <td>2.887088</td>\n",
       "      <td>9.122147</td>\n",
       "      <td>0.442015</td>\n",
       "      <td>0.218492</td>\n",
       "      <td>1.10235</td>\n",
       "      <td>5.965123</td>\n",
       "      <td>8.557321</td>\n",
       "      <td>2.462742</td>\n",
       "      <td>2.429297</td>\n",
       "      <td>3.990894</td>\n",
       "      <td>3.295158</td>\n",
       "      <td>0.965107</td>\n",
       "      <td>0.820834</td>\n",
       "      <td>1.036701</td>\n",
       "      <td>0.501255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>26.250000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.915000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>389.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>90.500000</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>34.500000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>620.250000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>232.250000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>55.250000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>4950.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>428.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5950.000000</td>\n",
       "      <td>926.000000</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       average_stars  review_count  compliment_cool  compliment_cute  \\\n",
       "count     200.000000    200.000000       200.000000            200.0   \n",
       "mean        3.688800      8.280000         0.170000              0.0   \n",
       "std         1.253741     16.662225         0.913935              0.0   \n",
       "min         1.000000      1.000000         0.000000              0.0   \n",
       "25%         3.000000      1.000000         0.000000              0.0   \n",
       "50%         3.915000      3.000000         0.000000              0.0   \n",
       "75%         5.000000      7.000000         0.000000              0.0   \n",
       "max         5.000000    154.000000         8.000000              0.0   \n",
       "\n",
       "       compliment_funny  compliment_hot  compliment_list  compliment_more  \\\n",
       "count        200.000000      200.000000            200.0       200.000000   \n",
       "mean           0.170000        0.080000              0.0         0.035000   \n",
       "std            0.913935        0.452353              0.0         0.306995   \n",
       "min            0.000000        0.000000              0.0         0.000000   \n",
       "25%            0.000000        0.000000              0.0         0.000000   \n",
       "50%            0.000000        0.000000              0.0         0.000000   \n",
       "75%            0.000000        0.000000              0.0         0.000000   \n",
       "max            8.000000        4.000000              0.0         4.000000   \n",
       "\n",
       "       compliment_note  compliment_photos  compliment_plain        cool  \\\n",
       "count       200.000000         200.000000        200.000000  200.000000   \n",
       "mean          0.150000           0.080000          0.250000    0.385000   \n",
       "std           0.787784           0.752617          0.944436    1.558499   \n",
       "min           0.000000           0.000000          0.000000    0.000000   \n",
       "25%           0.000000           0.000000          0.000000    0.000000   \n",
       "50%           0.000000           0.000000          0.000000    0.000000   \n",
       "75%           0.000000           0.000000          0.000000    0.000000   \n",
       "max           9.000000          10.000000          7.000000   15.000000   \n",
       "\n",
       "             fans       funny   rev_length   rev_stars     rev_use  \\\n",
       "count  200.000000  200.000000   200.000000  200.000000  200.000000   \n",
       "mean     0.630000    0.750000   534.855000    3.645000    0.705000   \n",
       "std      5.567593    4.385621   536.602446    1.662164    1.424119   \n",
       "min      0.000000    0.000000    47.000000    1.000000    0.000000   \n",
       "25%      0.000000    0.000000   216.000000    2.000000    0.000000   \n",
       "50%      0.000000    0.000000   389.000000    5.000000    0.000000   \n",
       "75%      0.000000    0.000000   620.250000    5.000000    1.000000   \n",
       "max     77.000000   56.000000  4950.000000    5.000000   10.000000   \n",
       "\n",
       "       friend_count  friend_label  rev_count_label   buss_star  buss_review  \\\n",
       "count    200.000000    200.000000       200.000000  200.000000   200.000000   \n",
       "mean      13.735000      0.195000         0.855000    3.762500   347.730000   \n",
       "std       58.262832      0.685071         0.660421    0.896584   769.966779   \n",
       "min        0.000000      0.000000         0.000000    1.000000     0.000000   \n",
       "25%        0.000000      0.000000         0.000000    3.500000    26.250000   \n",
       "50%        0.000000      0.000000         1.000000    4.000000    90.500000   \n",
       "75%        0.000000      0.000000         1.000000    4.500000   232.250000   \n",
       "max      428.000000      3.000000         4.000000    5.000000  5950.000000   \n",
       "\n",
       "       alpha_length  length_lemmatize          NN         NNP         NNS  \\\n",
       "count    200.000000        200.000000  200.000000  200.000000  200.000000   \n",
       "mean      98.620000         47.720000   19.630000    0.020000    3.935000   \n",
       "std      101.510975         46.028302   18.610612    0.172478    4.260462   \n",
       "min        9.000000          7.000000    2.000000    0.000000    0.000000   \n",
       "25%       39.000000         20.000000    8.000000    0.000000    1.000000   \n",
       "50%       72.500000         34.500000   14.000000    0.000000    3.000000   \n",
       "75%      113.000000         55.250000   22.250000    0.000000    5.000000   \n",
       "max      926.000000        407.000000  150.000000    2.000000   32.000000   \n",
       "\n",
       "              PDT         POS         PRP        PRP$          RB         RBR  \\\n",
       "count  200.000000  200.000000  200.000000  200.000000  200.000000  200.000000   \n",
       "mean     0.090000    0.180000    5.520000    2.420000    8.150000    0.160000   \n",
       "std      0.303911    0.537629    8.171503    2.887088    9.122147    0.442015   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    2.000000    0.000000    3.000000    0.000000   \n",
       "50%      0.000000    0.000000    3.000000    1.000000    6.000000    0.000000   \n",
       "75%      0.000000    0.000000    6.000000    3.000000   10.000000    0.000000   \n",
       "max      2.000000    4.000000   77.000000   15.000000   93.000000    3.000000   \n",
       "\n",
       "              RBS         RP          VB         VBD         VBG         VBN  \\\n",
       "count  200.000000  200.00000  200.000000  200.000000  200.000000  200.000000   \n",
       "mean     0.050000    0.53000    5.015000    7.220000    1.735000    1.945000   \n",
       "std      0.218492    1.10235    5.965123    8.557321    2.462742    2.429297   \n",
       "min      0.000000    0.00000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.00000    2.000000    2.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.00000    4.000000    5.000000    1.000000    1.000000   \n",
       "75%      0.000000    1.00000    6.000000    9.000000    2.000000    3.000000   \n",
       "max      1.000000    8.00000   52.000000   47.000000   15.000000   13.000000   \n",
       "\n",
       "              VBP         VBZ         WDT          WP         WRB       Faker  \n",
       "count  200.000000  200.000000  200.000000  200.000000  200.000000  200.000000  \n",
       "mean     2.680000    2.035000    0.385000    0.360000    0.525000    0.500000  \n",
       "std      3.990894    3.295158    0.965107    0.820834    1.036701    0.501255  \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000  \n",
       "25%      1.000000    0.000000    0.000000    0.000000    0.000000    0.000000  \n",
       "50%      2.000000    1.000000    0.000000    0.000000    0.000000    0.500000  \n",
       "75%      3.000000    3.000000    0.000000    0.000000    1.000000    1.000000  \n",
       "max     43.000000   38.000000    9.000000    4.000000    8.000000    1.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d={'Faker':[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\\\n",
    "            1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\\\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\\\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\\\n",
    "            1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}\n",
    "faker= pd.DataFrame(data=d)\n",
    "new = pd.concat([old_2,  revlen['rev_length'] , revlen['rev_stars'], revlen['rev_use'], frnd, Xlen_label, \\\n",
    "                 businessstars, Alphalength, lemmatizewords, postotal, faker], axis=1)\n",
    "#\n",
    "new['buss_star'].replace(to_replace= 'NaN', value=np.nan, inplace=True)\n",
    "new['buss_star'].fillna(value= 1, inplace=True)  # business not found \n",
    "new['buss_review'].replace(to_replace= 'NaN', value=np.nan, inplace=True)\n",
    "new['buss_review'].fillna(value= 0, inplace=True)  # business not found \n",
    "cleaned_data=new\n",
    "dataset1=cleaned_data[cleaned_data['Faker']==1]\n",
    "dataset0=cleaned_data[cleaned_data['Faker']==0]\n",
    "\n",
    "cleaned_data.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.to_csv('cleaned_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
